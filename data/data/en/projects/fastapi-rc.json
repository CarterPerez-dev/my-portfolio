{
  "slug": "fastapi-rc",
  "language": "en",
  "title": "fastapi-rc",
  "subtitle": "Redis caching for FastAPI without the decorator magic",
  "description": "A three-tier Redis caching library for FastAPI that gives you direct client access, a generic service wrapper, and custom domain caches that integrate with FastAPI's dependency injection. Built for developers who want granular control over caching behavior rather than opinionated decorators.",
  "technical_details": "The library is structured around three abstraction levels that you can mix freely. At the bottom, you get the raw async Redis client through FastAPI's DI system. One layer up, CacheService wraps common patterns like cache-aside, TTL management, and serialization. At the top, you define typed per-domain caches as dependencies. This lets you drop down to raw Redis for pipelines or Lua scripts while still using the service layer for 90% of operations.\n\n```python\n# All three levels in one endpoint\n@router.post(\"/orders\")\nasync def create_order(\n    user_cache: UserCache,        # Domain-specific CacheService\n    product_cache: ProductCache,  # Another domain cache\n    redis: RedisClient,           # Raw client for custom ops\n):\n    user = await user_cache.get_or_set(user_id, factory=fetch_user)\n    async with redis.pipeline() as pipe:  # Drop to raw for batch\n        ...\n```\n\nCacheService is generic over Pydantic models, so `CacheService[User]` actually validates and deserializes cached data back into User instances. The serialization path checks if the value is a BaseModel and calls `model_dump_json()`, otherwise falls back to standard JSON encoding. On read, if a model type is configured, it runs `model_validate_json()` on the cached string. This catches schema drift early rather than blowing up downstream when you access an attribute that no longer exists.\n\n```python\nclass CacheService(Generic[T]):\n    def __init__(\n        self,\n        redis: Redis,\n        namespace: str,\n        model: type[T] | None = None,  # Optional Pydantic model\n        default_ttl: int = 300,\n        use_jitter: bool = True,\n        prefix: str = \"cache\",\n        version: str = \"v1\",\n    ): ...\n```\n\nCache keys are generated deterministically with collision resistance. The format is `{prefix}:{version}:{namespace}:{identifier}` with an optional param hash appended. When you pass query parameters to `get_or_set`, they get JSON serialized with sorted keys, then SHA256 hashed to 12 characters. This means `get_or_set(\"list\", params={\"page\": 1, \"category\": \"electronics\"})` produces a stable key regardless of dict ordering. The version segment exists specifically for cache invalidation during deployments.\n\n```python\ndef build_cache_key(\n    namespace: str,\n    identifier: str,\n    params: dict[str, Any] | None = None,\n    prefix: str = \"cache\",\n    version: str = \"v1\",\n) -> str:\n    parts = [prefix, version, namespace, identifier]\n    if params:\n        param_str = json.dumps(params, sort_keys=True)\n        param_hash = hashlib.sha256(param_str.encode()).hexdigest()[:12]\n        parts.append(param_hash)\n    return \":\".join(parts)\n```\n\nTTL jitter is on by default because cache stampedes are a real problem that most tutorials ignore. When a thousand requests hit an expired popular key simultaneously, they all miss cache and slam your database. The `get_ttl_with_jitter` function adds random variance (default 10%) to spread expiration times. A 300 second TTL becomes something between 270 and 330 seconds. Combined with the cache-aside pattern in `get_or_set`, this prevents synchronized expiration across your hot keys.\n\n```python\ndef get_ttl_with_jitter(base_ttl: int, jitter_percent: float = 0.1) -> int:\n    jitter = int(base_ttl * jitter_percent)\n    return base_ttl + random.randint(-jitter, jitter)\n```\n\nConnection management uses redis-py's async connection pool with retry logic baked in. The retry strategy is `ExponentialWithJitterBackoff` capped at 10 seconds, handling ConnectionError, TimeoutError, and BusyLoadingError specifically. Pattern invalidation uses SCAN instead of KEYS because KEYS blocks the Redis server on large keyspaces. The `invalidate_pattern` method iterates with `scan_iter` and deletes matches individually. It's slower than a single KEYS + DEL but won't freeze your production Redis for 30 seconds.",
  "tech_stack": [
    "Python 3.12",
    "FastAPI",
    "Redis",
    "Pydantic v2",
    "hiredis",
    "redis-py async"
  ],
  "github_url": "https://github.com/CarterPerez-dev/fastapi-rc",
  "demo_url": null,
  "website_url": null,
  "docs_url": null,
  "blog_url": null,
  "pypi_url": "https://pypi.org/project/fastapi-rc/",
  "npm_url": null,
  "ios_url": null,
  "android_url": null,
  "code_snippet": "class CacheService(Generic[T]):\n    \"\"\"\n    Generic caching service for Pydantic models\n    Provides cache-aside pattern with automatic serialization\n    \"\"\"\n    def __init__(\n        self,\n        redis: Redis,\n        namespace: str,\n        model: type[T] | None = None,\n        default_ttl: int = 300,\n        use_jitter: bool = True,\n        prefix: str = \"cache\",\n        version: str = \"v1\",\n    ):\n        self.redis = redis\n        self.namespace = namespace\n        self.model = model\n        self.default_ttl = default_ttl\n        self.use_jitter = use_jitter\n        self.prefix = prefix\n        self.version = version\n\n    async def get_or_set(\n        self,\n        identifier: str,\n        factory: Callable[[], Awaitable[T]],\n        ttl: int | None = None,\n        params: dict[str, Any] | None = None,\n    ) -> T:\n        \"\"\"\n        Cache-aside pattern: get from cache or execute factory and cache result\n        \"\"\"\n        cached = await self.get(identifier, params)\n        if cached is not None:\n            return cached\n\n        value = await factory()\n        await self.set(identifier, value, ttl, params)\n        return value",
  "code_language": "python",
  "code_filename": "https://github.com/CarterPerez-dev/fastapi-rc/blob/main/fastapi_rc/service.py",
  "thumbnail_url": null,
  "banner_url": null,
  "screenshots": null,
  "stars_count": 3,
  "forks_count": null,
  "downloads_count": null,
  "users_count": null,
  "display_order": 0,
  "is_complete": true,
  "is_featured": false,
  "status": "active",
  "start_date": "2025-12-13",
  "end_date": null
}
