{
  "slug": "vuemantics",
  "language": "en",
  "title": "Vuemantics",
  "subtitle": "Semantic image search using vector embeddings and multimodal AI",
  "description": "I built this to actually understand how semantic search works, not just call an API and hope for the best. The idea came from frustration with Apple Photos. It's 2025 and if I search 'friend in black jacket' it finds nothing useful because it's still just matching text metadata. Meanwhile I can build actual semantic search myself? Something felt wrong about that gap.",
  "technical_details": "The architecture chains two AI models together, each doing what it's good at. Gemini Pro handles vision, analyzing uploaded images and videos to generate detailed text descriptions (tone, subjects, setting, colors, actions). Those descriptions then get fed to OpenAI's text-embedding-3-small which converts them into 1536-dimensional vectors. The vectors live in PostgreSQL via pgvector, and similarity search happens through cosine distance calculations.\n\n```python\n# The core pipeline: vision model describes, embedding model vectorizes\ndescription = await self._analyze_image_with_gemini(file_path)\nembedding = await self._generate_embedding(description)\nawait upload.update_analysis(gemini_summary=description, embedding=embedding)\n```\n\nSearch queries go through the same embedding process. You type 'sunset at the beach', that text becomes a vector, and pgvector finds the images whose description vectors are closest in that 1536-dimensional space. The math is just cosine similarity but the results feel like magic when it works.\n\n```python\n# Query text becomes a vector, pgvector finds nearest neighbors\nquery_embedding = await self._generate_query_embedding(request.query)\nresults = await Upload.search_by_embedding(\n    query_embedding=query_embedding,\n    user_id=user_id,\n    similarity_threshold=request.similarity_threshold\n)\n```\n\nThe backend uses FastAPI with fully async processing. Uploads return immediately while a background task handles the Gemini → OpenAI → pgvector pipeline. I went with raw asyncpg instead of an ORM because I needed direct control over the vector type codec registration. Redis handles caching. The frontend is React with TypeScript, nothing fancy, just functional.\n\n```python\n# Custom vector codec for asyncpg - pgvector doesn't have native Python support\nawait conn.set_type_codec(\n    \"vector\",\n    encoder=lambda v: f\"[{','.join(map(str, v))}]\",\n    decoder=lambda v: list(map(float, v[1:-1].split(\",\"))),\n    schema=\"public\",\n)\n```\n\nCurrent status: it works, but needs threshold calibration. There's a similarity_threshold parameter (defaulting to 0.25) that controls how strict the matching is. Too low and you get irrelevant results, too high and you miss valid matches. I got it to a decent stopping point and just never circled back to fine tune it properly. The hard parts are done, it's just parameter tweaking left.\n\nNext step is running this on my home server with local models instead of paid APIs. Ollama now supports vision models (llava, llama3.2-vision, qwen2-vl) that can replace Gemini for generating descriptions. For embeddings, nomic-embed-text or mxbai-embed-large work locally. Quality drops a bit but for personal media search it's fine, and then it's actually free to run. Eventually I want to build an iOS app on top of it. Basically my own private iCloud Photos that can actually find things.",
  "tech_stack": [
    "Python",
    "FastAPI",
    "PostgreSQL",
    "pgvector",
    "Redis",
    "React",
    "TypeScript",
    "Vite",
    "Tailwind CSS",
    "OpenAI API",
    "Google Gemini API",
    "Docker",
    "Nginx"
  ],
  "github_url": "https://github.com/CarterPerez-dev/vuemantics",
  "website_url": "https://vuemantics.com",
  "demo_url": null,
  "docs_url": null,
  "blog_url": null,
  "pypi_url": null,
  "npm_url": null,
  "ios_url": null,
  "android_url": null,
  "code_snippet": "async def analyze_media(self, upload_id: UUID) -> None:\n    \"\"\"\n    Analyze media file and generate embeddings.\n    Updates upload record with gemini_summary, embedding, and processing_status.\n    \"\"\"\n    upload = await Upload.find_by_id(upload_id)\n    if not upload:\n        return\n\n    await upload.update_status(ProcessingStatus.ANALYZING)\n\n    # Get description from Gemini (vision model)\n    if upload.file_type == \"image\":\n        description = await self._analyze_image_with_gemini(file_path)\n    else:\n        description = await self._analyze_video_with_gemini(\n            upload.user_id, upload_id, file_path\n        )\n\n    await upload.update_status(ProcessingStatus.EMBEDDING)\n\n    # Convert description to vector embedding (OpenAI)\n    embedding = await self._generate_embedding(description)\n\n    # Store both for search\n    await upload.update_analysis(\n        gemini_summary=description, \n        embedding=embedding\n    )",
  "code_language": "python",
  "code_filename": "https://github.com/CarterPerez-dev/vuemantics/tree/main/backend/services/ai_service.py",
  "thumbnail_url": null,
  "banner_url": null,
  "screenshots": null,
  "stars_count": null,
  "forks_count": null,
  "downloads_count": null,
  "users_count": null,
  "display_order": 0,
  "is_complete": false,
  "is_featured": false,
  "status": "active",
  "start_date": "2025-06-10",
  "end_date": null
}
