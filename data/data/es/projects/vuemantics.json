{
  "slug": "vuemantics",
  "language": "es",
  "title": "Vuemantics",
  "subtitle": "Búsqueda semántica de imágenes usando vector embeddings e IA multimodal",
  "description": "Construí esto para realmente entender cómo funciona la búsqueda semántica, no solo llamar a una API y esperar lo mejor. La idea surgió de la frustración con Apple Photos. Es 2025 y si busco 'amigo con chaqueta negra' no encuentra nada útil porque sigue haciendo matching solo con metadatos de texto. Mientras tanto, ¿yo puedo construir búsqueda semántica real? Algo se sentía mal con esa brecha.",
  "technical_details": "La arquitectura encadena dos modelos de IA, cada uno haciendo lo que hace bien. Gemini Pro maneja visión, analizando imágenes y videos subidos para generar descripciones detalladas de texto (tono, sujetos, escenario, colores, acciones). Esas descripciones luego se alimentan a text-embedding-3-small de OpenAI que las convierte en vectores de 1536 dimensiones. Los vectores viven en PostgreSQL via pgvector, y la búsqueda por similitud ocurre mediante cálculos de distancia coseno.\n\n```python\n# El pipeline central: modelo de visión describe, modelo de embedding vectoriza\ndescription = await self._analyze_image_with_gemini(file_path)\nembedding = await self._generate_embedding(description)\nawait upload.update_analysis(gemini_summary=description, embedding=embedding)\n```\n\nLas consultas de búsqueda pasan por el mismo proceso de embedding. Escribes 'atardecer en la playa', ese texto se convierte en vector, y pgvector encuentra las imágenes cuyas descripciones son vectorialmente más cercanas en ese espacio de 1536 dimensiones. La matemática es solo similitud coseno pero los resultados se sienten como magia cuando funciona.\n\n```python\n# El texto de consulta se convierte en vector, pgvector encuentra vecinos más cercanos\nquery_embedding = await self._generate_query_embedding(request.query)\nresults = await Upload.search_by_embedding(\n    query_embedding=query_embedding,\n    user_id=user_id,\n    similarity_threshold=request.similarity_threshold\n)\n```\n\nEl backend usa FastAPI con procesamiento completamente async. Las subidas retornan inmediatamente mientras una tarea en background maneja el pipeline Gemini → OpenAI → pgvector. Fui con asyncpg raw en lugar de un ORM porque necesitaba control directo sobre el registro del codec del tipo vector. Redis maneja el caching. El frontend es React con TypeScript, nada fancy, solo funcional.\n\n```python\n# Codec de vector personalizado para asyncpg - pgvector no tiene soporte nativo de Python\nawait conn.set_type_codec(\n    \"vector\",\n    encoder=lambda v: f\"[{','.join(map(str, v))}]\",\n    decoder=lambda v: list(map(float, v[1:-1].split(\",\"))),\n    schema=\"public\",\n)\n```\n\nEstado actual: funciona, pero necesita calibración de threshold. Hay un parámetro similarity_threshold (por defecto 0.25) que controla qué tan estricto es el matching. Muy bajo y obtienes resultados irrelevantes, muy alto y pierdes matches válidos. Lo dejé en un punto decente y simplemente nunca volví a ajustarlo bien. Las partes difíciles están hechas, solo falta ajuste de parámetros.\n\nEl siguiente paso es correr esto en mi servidor casero con modelos locales en lugar de APIs pagas. Ollama ahora soporta modelos de visión (llava, llama3.2-vision, qwen2-vl) que pueden reemplazar a Gemini para generar descripciones. Para embeddings, nomic-embed-text o mxbai-embed-large funcionan localmente. La calidad baja un poco pero para búsqueda de medios personales está bien, y entonces es realmente gratis de correr. Eventualmente quiero construir una app iOS encima. Básicamente mi propio iCloud Photos privado que realmente puede encontrar cosas.",
  "tech_stack": [
    "Python",
    "FastAPI",
    "PostgreSQL",
    "pgvector",
    "Redis",
    "React",
    "TypeScript",
    "Vite",
    "Tailwind CSS",
    "OpenAI API",
    "Google Gemini API",
    "Docker",
    "Nginx"
  ],
  "github_url": "https://github.com/CarterPerez-dev/vuemantics",
  "website_url": "https://vuemantics.com",
  "demo_url": null,
  "docs_url": null,
  "blog_url": null,
  "pypi_url": null,
  "npm_url": null,
  "ios_url": null,
  "android_url": null,
  "code_snippet": "async def analyze_media(self, upload_id: UUID) -> None:\n    \"\"\"\n    Analyze media file and generate embeddings.\n    Updates upload record with gemini_summary, embedding, and processing_status.\n    \"\"\"\n    upload = await Upload.find_by_id(upload_id)\n    if not upload:\n        return\n\n    await upload.update_status(ProcessingStatus.ANALYZING)\n\n    # Get description from Gemini (vision model)\n    if upload.file_type == \"image\":\n        description = await self._analyze_image_with_gemini(file_path)\n    else:\n        description = await self._analyze_video_with_gemini(\n            upload.user_id, upload_id, file_path\n        )\n\n    await upload.update_status(ProcessingStatus.EMBEDDING)\n\n    # Convert description to vector embedding (OpenAI)\n    embedding = await self._generate_embedding(description)\n\n    # Store both for search\n    await upload.update_analysis(\n        gemini_summary=description, \n        embedding=embedding\n    )",
  "code_language": "python",
  "code_filename": "https://github.com/CarterPerez-dev/vuemantics/tree/main/backend/services/ai_service.py",
  "thumbnail_url": null,
  "banner_url": null,
  "screenshots": null,
  "stars_count": null,
  "forks_count": null,
  "downloads_count": null,
  "users_count": null,
  "display_order": 0,
  "is_complete": false,
  "is_featured": false,
  "status": "active",
  "start_date": "2025-06-10",
  "end_date": null
}
