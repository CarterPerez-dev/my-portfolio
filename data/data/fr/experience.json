[
  {
    "language": "fr",
    "company": "Cyber Skyline",
    "company_url": "https://cyberskyline.com",
    "company_logo_url": null,
    "location": "Télétravail",
    "role": "Ingénieur Logiciel",
    "department": "Backend",
    "employment_type": "full_time",
    "start_date": "2025-05-01",
    "end_date": null,
    "is_current": true,
    "description": "Ingénieur backend pour la plateforme de la Compétition CTF annuelle President's Cup de CISA. J'ai rejoint une équipe de 7 personnes et suis devenu le principal contributeur backend, construisant l'infrastructure centrale de la compétition incluant le moteur de scoring, le système de notifications en temps réel, le système de tickets de support et la gestion des équipes/événements.\n\n## Le Problème Multi-Conteneur\n\nLa plateforme exécute plusieurs conteneurs Flask derrière un load balancer sur Docker Swarm. Les implémentations WebSocket standard échouent dans cette configuration car les rooms Flask-SocketIO sont en mémoire par processus. Si un utilisateur se connecte au Conteneur 1 et qu'une mise à jour de score se produit sur le Conteneur 3, cet utilisateur ne reçoit jamais la notification. Chaque fonctionnalité temps réel nécessitait une solution fonctionnant au-delà des frontières des conteneurs.\n\n## Coordination Redis Pub/Sub\n\nLa solution : chaque conteneur maintient sa propre table de correspondance mappant les IDs utilisateur aux IDs de session socket. Quand une notification se déclenche, elle est publiée sur un canal Redis. Chaque conteneur reçoit le message, vérifie \"cet utilisateur est-il connecté à moi ?\", et n'émet que s'il a une connexion active. Pas de broadcasts gaspillés, pas de gestion de rooms inter-conteneurs.\n\n```python\ndef _handle_notification_message(self, message):\n    user_ids = message.get('user_ids', [])\n    event_name = message.get('event_name')\n    data = message.get('data')\n\n    for user_id in user_ids:\n        user_sids = get_user_connections(user_id)\n        for sid in user_sids:\n            self.socketio.emit(event_name, data, to=sid)\n```\n\n## Le Bug de Threading Gunicorn\n\nEn développement, tout fonctionnait. En production avec les workers gevent de Gunicorn, le thread subscriber Redis mourait silencieusement. Deux problèmes combinés : le modèle pre-fork de Gunicorn détruit les threads démarrés pendant l'initialisation de l'app, et le timing du monkey patching de gevent diffère entre dev et prod. La solution était de remplacer `threading.Thread` par `socketio.start_background_task()`, qui sélectionne automatiquement `gevent.spawn()` en mode gevent. Petit changement, des heures de debugging pour le trouver.\n\n## Abstraire la Complexité\n\nLe code métier ne devrait pas connaître Redis ou la coordination de conteneurs. Le `NotificationService` expose des méthodes simples comme `broadcast_attempt_update(event_id, team_id, challenge_id)`. En interne, il résout les IDs utilisateur, publie sur Redis, gère les échecs gracieusement. Un collègue travaillant sur le contrôleur de scoring appelle simplement la méthode et passe à la suite.\n\n## Middleware Basé sur Décorateurs\n\nChaque endpoint nécessite auth, validation d'entrée, chargement de ressources et vérification de permissions. Le pattern qui a émergé à travers les code reviews et l'itération avec l'équipe : des décorateurs composables qui s'empilent proprement.\n\n```python\n@user_endpoint(json_required=True)\n@load_event(LoaderType.PARAM)\n@load_challenge(LoaderType.PARAM)\n@load_question(LoaderType.PARAM)\n@load_team_by_user_and_event()\n@check_permissions(PermissionEnum.CAN_PLAY_CHALLENGES)\ndef post(self, event_id, challenge_id, question_id,\n         event, challenge, question, team, current_user, json_data):\n    return submit_answer(event, challenge, question, team, current_user, json_data)\n```\n\nQuand le handler s'exécute, l'auth est vérifiée, le JSON est parsé, tous les modèles sont chargés, les permissions sont vérifiées. Le handler ne fait que la logique métier. Ce pattern s'est répandu dans tout le backend via les PRs et le feedback.\n\n## Ce Que J'ai Appris\n\nC'était ma première fois à travailler sur quelque chose de cette envergure avec une vraie équipe. Les devs seniors, particulièrement ceux gérant le DevOps et l'orchestration Docker Swarm, m'ont appris comment vraiment penser les systèmes distribués. Les patterns de décorateurs sont venus du feedback de code review. En comparant mes premiers PRs aux plus récents, la progression est évidente. Livrer du code que d'autres personnes doivent maintenir change votre façon de l'écrire.",
    "responsibilities": null,
    "achievements": [
      "Contributeur #1 (70K+ lignes)",
      "8 domaines complets construits",
      "50+ endpoints API",
      "70% de la suite de tests",
      "Coordination WebSocket multi-conteneur"
    ],
    "tech_stack": ["Python", "Flask", "Flask-SocketIO", "Redis", "PostgreSQL", "SQLAlchemy", "AWS SES", "AWS S3", "Docker", "pytest"],
    "display_order": 1,
    "is_visible": true
  },
  {
    "language": "fr",
    "company": "Sealing Technologies",
    "company_url": "https://sealingtech.com",
    "company_logo_url": null,
    "location": "Columbia, MD",
    "role": "Technicien d'Intégration II",
    "department": "Production",
    "employment_type": "full_time",
    "start_date": "2025-05-01",
    "end_date": "2025-05-01",
    "is_current": false,
    "description": "Assurance qualité et intégration système pour des systèmes de défense personnalisés, incluant serveurs, switches réseau et Cyber-Fly-Away Kits construits selon les spécifications clients. J'ai travaillé dans des workflows certifiés ISO 9001:2015 pour valider le matériel avant déploiement aux clients gouvernementaux et de défense.\n\nLe processus QA impliquait la vérification des configurations BIOS, firmware BMC, mémoire système et métriques de santé du stockage. Les tests de stress avec Prime95 poussaient les CPUs aux limites thermiques tout en surveillant les températures et vitesses de ventilateur. FIO benchmarkait les I/O de stockage pour confirmer que les disques géraient les opérations d'écriture soutenues sans dégradation. Les cartes d'interface réseau étaient validées pour le débit attendu. Chaque système passait par des vérifications documentées avant expédition.\n\nAu-delà du QA pratique, j'ai rédigé des blogs techniques pour le site web de l'entreprise expliquant nos processus d'assemblage et de test, et développé des SOPs pour standardiser les procédures entre équipes et fluidifier l'intégration des nouveaux techniciens.",
    "responsibilities": null,
    "achievements": [
      "Conseillé sur 40M$+ de matériel serveur/réseau",
      "Conformité ISO 9001:2015",
      "Rédigé blogs techniques de l'entreprise",
      "Développé SOPs d'équipe"
    ],
    "tech_stack": ["RHEL", "SELinux", "Prime95", "FIO", "BMC/BIOS", "Hardware Diagnostics"],
    "display_order": 2,
    "is_visible": true
  },
  {
    "language": "fr",
    "company": "Jimmy John's",
    "company_url": null,
    "company_logo_url": null,
    "location": "Severna Park, MD",
    "role": "Directeur Général",
    "department": null,
    "employment_type": "full_time",
    "start_date": "2022-12-01",
    "end_date": "2025-06-01",
    "is_current": false,
    "description": "Avant la tech, je dirigeais un restaurant à haut volume. J'ai géré une équipe de 10+, géré les opérations quotidiennes, optimisé les workflows pour réduire les coûts de main-d'œuvre, et atteint constamment les objectifs de performance. Je suis devenu le GM le plus performant d'une franchise de 5 magasins. Le travail m'a appris à diriger sous pression, prioriser quand tout semble urgent, et assumer la responsabilité des résultats. Industrie différente, mêmes fondamentaux.",
    "responsibilities": null,
    "achievements": [
      "GM le plus performant",
      "Géré 10+ employés",
      "Réduit les temps de livraison de 8 min/semaine"
    ],
    "tech_stack": null,
    "display_order": 3,
    "is_visible": true
  }
]
