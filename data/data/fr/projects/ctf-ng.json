{
  "slug": "cisa-presidents-cup-ctf",
  "language": "fr",
  "title": "Plateforme CTF President's Cup CISA",
  "subtitle": "Backend de plateforme CTF distribuée avec notation temps réel et coordination WebSocket multi-conteneurs",
  "description": "Contributeur backend principal pour la plateforme de compétition cybersécurité annuelle President's Cup de CISA. Conçu et implémenté l'architecture fondamentale incluant le système de notification temps réel, moteur de notation, ticketing de support et patterns middleware adoptés à travers la codebase. La plateforme gère les utilisateurs concurrents à travers des conteneurs Flask distribués dans un déploiement Docker Swarm.",
  "technical_details": "## Le Problème de Coordination Distribuée\n\nLa plateforme exécute plusieurs conteneurs Flask derrière un load balancer. Les implémentations WebSocket standard cassent dans cette configuration parce que les salles Flask-SocketIO sont en mémoire par processus. Si l'Utilisateur A se connecte au Conteneur 1 et l'Utilisateur B se connecte au Conteneur 2, ils ne peuvent pas communiquer via salles. Chaque fonctionnalité temps réel (mises à jour tableau de classement, notifications de notation, réponses ticket support) nécessitait une solution fonctionnant à travers les frontières de conteneurs.\n\n## Redis Pub/Sub comme Couche de Coordination\n\nLa solution: chaque conteneur maintient sa propre table de correspondance mappant IDs utilisateur à IDs de session socket. Quand une notification se déclenche, elle publie vers un canal Redis. Chaque conteneur reçoit le message, vérifie \"cet utilisateur est-il connecté à moi?\", et émet seulement s'il a une connexion active. Pas de broadcasts gaspillés, pas de gestion de salle cross-conteneur.\n\n```python\ndef _handle_notification_message(self, message):\n    user_ids = message.get('user_ids', [])\n    event_name = message.get('event_name')\n    data = message.get('data')\n\n    for user_id in user_ids:\n        user_sids = get_user_connections(user_id)  # Recherche locale seulement\n        for sid in user_sids:\n            self.socketio.emit(event_name, data, to=sid)\n```\n\nLe dict `user_connections` est par conteneur. Redis gère le broadcast, chaque conteneur gère ses propres utilisateurs. Séparation propre.\n\n## Le Bug de Threading Gunicorn\n\nEn développement, tout fonctionnait. En production avec les workers gevent de Gunicorn, le thread abonné Redis mourait silencieusement. Deux problèmes cumulés: le modèle pre-fork de Gunicorn détruit les threads démarrés pendant l'initialisation de l'app, et le timing du monkey patching de gevent diffère entre dev et prod. La solution était de remplacer `threading.Thread` par `socketio.start_background_task()`, qui auto-sélectionne `gevent.spawn()` en mode gevent. Petit changement, des heures de débogage pour le trouver.\n\n## Abstraction de la Complexité\n\nLe code de domaine ne devrait pas connaître Redis ou la coordination de conteneurs. Le `NotificationService` expose des méthodes simples comme `broadcast_attempt_update(event_id, team_id, challenge_id)`. Sous le capot il résout les IDs utilisateur, publie vers Redis, gère les échecs gracieusement. Un coéquipier travaillant sur le contrôleur de notation appelle juste la méthode et continue.\n\n```python\n@staticmethod\ndef broadcast_attempt_update(event_id, team_id, challenge_id, question_id):\n    NotificationService._emit_refetch(\n        path=f\"/ng/events/{event_id}/challenges/{challenge_id}\",\n        team_id=team_id\n    )\n    NotificationService._emit_refetch(\n        path=f\"/ng/events/{event_id}/leaderboard\",\n        event_id=event_id\n    )\n```\n\n## Middleware Basé sur Décorateurs\n\nChaque endpoint nécessite auth, validation d'entrée, chargement de ressources et vérifications de permissions. Écrire ce boilerplate dans chaque gestionnaire de route est sujet aux erreurs et moche. L'approche qui a émergé (à travers beaucoup de feedback PR et itération avec l'équipe): décorateurs composables qui s'empilent proprement.\n\n```python\n@user_endpoint(json_required=True)\n@load_event(LoaderType.PARAM)\n@load_challenge(LoaderType.PARAM)\n@load_question(LoaderType.PARAM)\n@load_team_by_user_and_event()\n@check_permissions(PermissionEnum.CAN_PLAY_CHALLENGES)\ndef post(self, event_id, challenge_id, question_id,\n         event, challenge, question, team, current_user, json_data):\n    return submit_answer(event, challenge, question, team, current_user, json_data)\n```\n\nAu moment où le gestionnaire s'exécute, l'auth est vérifiée, le JSON est parsé, tous les modèles sont chargés, les permissions sont vérifiées. Le gestionnaire fait juste la logique métier. Ce pattern s'est répandu à travers tout le backend. Chargeurs de ressources pour événements, équipes, tickets, scores. Décorateurs de permissions. Validateurs de propriété. Tout composable.\n\n## Framework de Validation\n\nChaque modèle nécessite validation d'entrée avec messages d'erreur cohérents. Au lieu de vérifications ad-hoc dispersées à travers les contrôleurs, il y a un `BaseValidator` avec méthodes chaînables et un décorateur `@validation_field` qui gère les patterns communs (vérifications requises, gestion None, messages d'erreur conviviaux).\n\n```python\nvalidator = BaseValidator()\nvalidator.validate_string(data, \"name\", max_length=64, required=True)\nvalidator.validate_model_id(data, \"event_id\", \"Event\", required=True)\nvalidator.validate_datetime(data, \"expires_at\", allow_past=False)\nreturn validator.validate()  # Retourne dict propre ou lève ValidationError\n```\n\nPasse unique: valide et extrait les données parsées. La méthode de classe `validate()` de chaque modèle utilise ceci. Forme d'erreur cohérente à travers toute l'API.\n\n## Ce Que J'ai Appris\n\nC'était ma première fois à travailler sur quelque chose à cette échelle avec une vraie équipe. Les devs seniors sur le projet (particulièrement ceux gérant DevOps, l'orchestration Docker Swarm complexe, parsing de défis, gestion VNC et plus) m'ont appris comment réellement penser aux systèmes distribués, pas juste construire des fonctionnalités. Les patterns de décorateurs sont sortis du feedback de revue de code. Le framework de validation a évolué à travers plusieurs itérations basées sur ce dont l'équipe avait réellement besoin. Regarder mes premières PRs versus les plus tardives, la croissance est embarrassante mais réelle. Livrer du code que d'autres personnes doivent maintenir change comment vous l'écrivez.",
  "tech_stack": ["Python", "Flask", "Flask-SocketIO", "Redis", "PostgreSQL", "SQLAlchemy", "AWS SES", "AWS S3", "CTFd", "Docker Swarm", "pytest", "Nginx"],
  "github_url": "https://github.com/CyberSkyline/ctf-ng",
  "website_url": "https://presidentscup.cisa.gov/",
  "demo_url": null,
  "docs_url": null,
  "blog_url": null,
  "pypi_url": null,
  "npm_url": null,
  "ios_url": null,
  "android_url": null,
  "code_snippet": "@user_endpoint(json_required=True)\n@load_event(LoaderType.PARAM)\n@load_challenge(LoaderType.PARAM)\n@load_question(LoaderType.PARAM)\n@load_team_by_user_and_event()\n@limiter.limit(\"1 per 1 seconds\")\n@check_permissions(PermissionEnum.CAN_PLAY_CHALLENGES, \"You do not have permission to play challenges.\")\ndef post(\n    self,\n    event_id: int,\n    challenge_id: int,\n    question_id: int,\n    event,\n    challenge,\n    question,\n    team,\n    current_user: User,\n    permissions,\n    json_data,\n    **kwargs,\n):\n    result = submit_answer(\n        event=event,\n        challenge=challenge,\n        question=question,\n        team=team,\n        current_user=current_user,\n        submission=json_data.get(\"submission\", \"\"),\n    )\n    return success_response(result, status_code=201)",
  "code_language": "python",
  "code_filename": "https://github.com/CyberSkyline/ctf-ng/blob/development/backend/ng/scoring/routes/user_routes.py",
  "display_order": 0,
  "is_complete": true,
  "is_featured": true,
  "thumbnail_url": null,
  "banner_url": null,
  "screenshots": null,
  "stars_count": 3,
  "forks_count": null,
  "downloads_count": null,
  "users_count": null,
  "status": "active",
  "start_date": "2025-05-01",
  "end_date": null
}
