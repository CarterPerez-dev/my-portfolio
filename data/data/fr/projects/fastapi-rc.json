{
  "slug": "fastapi-rc",
  "language": "fr",
  "title": "fastapi-rc",
  "subtitle": "Mise en cache Redis pour FastAPI sans la magie des décorateurs",
  "description": "Une bibliothèque de mise en cache Redis à trois niveaux pour FastAPI qui vous donne un accès direct au client, un wrapper de service générique et des caches de domaine personnalisés qui s'intègrent avec l'injection de dépendances de FastAPI. Construit pour les développeurs qui veulent un contrôle granulaire du comportement de mise en cache plutôt que des décorateurs orientés.",
  "technical_details": "La bibliothèque est structurée autour de trois niveaux d'abstraction que vous pouvez mélanger librement. En bas, vous obtenez le client Redis async brut via le système DI de FastAPI. Un niveau au-dessus, CacheService encapsule les patterns communs comme cache-aside, gestion TTL et sérialisation. Au sommet, vous définissez des caches typés par domaine comme dépendances. Cela vous permet de descendre au Redis brut pour des pipelines ou des scripts Lua tout en utilisant la couche service pour 90% des opérations.\n\n```python\n# Les trois niveaux dans un endpoint\n@router.post(\"/orders\")\nasync def create_order(\n    user_cache: UserCache,        # CacheService spécifique au domaine\n    product_cache: ProductCache,  # Un autre cache de domaine\n    redis: RedisClient,           # Client brut pour ops personnalisées\n):\n    user = await user_cache.get_or_set(user_id, factory=fetch_user)\n    async with redis.pipeline() as pipe:  # Descendre au brut pour batch\n        ...\n```\n\nCacheService est générique sur les modèles Pydantic, donc `CacheService[User]` valide et désérialise réellement les données en cache en instances User. Le chemin de sérialisation vérifie si la valeur est un BaseModel et appelle `model_dump_json()`, sinon revient à l'encodage JSON standard. En lecture, si un type de modèle est configuré, il exécute `model_validate_json()` sur la chaîne en cache. Cela détecte tôt la dérive de schéma plutôt que d'exploser en aval quand vous accédez à un attribut qui n'existe plus.\n\n```python\nclass CacheService(Generic[T]):\n    def __init__(\n        self,\n        redis: Redis,\n        namespace: str,\n        model: type[T] | None = None,  # Modèle Pydantic optionnel\n        default_ttl: int = 300,\n        use_jitter: bool = True,\n        prefix: str = \"cache\",\n        version: str = \"v1\",\n    ): ...\n```\n\nLes clés de cache sont générées de manière déterministe avec résistance aux collisions. Le format est `{prefix}:{version}:{namespace}:{identifier}` avec un hash de param optionnel ajouté. Quand vous passez des paramètres de requête à `get_or_set`, ils sont sérialisés JSON avec des clés triées, puis hachés SHA256 à 12 caractères. Cela signifie `get_or_set(\"list\", params={\"page\": 1, \"category\": \"electronics\"})` produit une clé stable indépendamment de l'ordre du dict. Le segment version existe spécifiquement pour l'invalidation de cache lors des déploiements.\n\n```python\ndef build_cache_key(\n    namespace: str,\n    identifier: str,\n    params: dict[str, Any] | None = None,\n    prefix: str = \"cache\",\n    version: str = \"v1\",\n) -> str:\n    parts = [prefix, version, namespace, identifier]\n    if params:\n        param_str = json.dumps(params, sort_keys=True)\n        param_hash = hashlib.sha256(param_str.encode()).hexdigest()[:12]\n        parts.append(param_hash)\n    return \":\".join(parts)\n```\n\nLe jitter TTL est activé par défaut parce que les cache stampedes sont un vrai problème que la plupart des tutoriels ignorent. Quand mille requêtes frappent une clé populaire expirée simultanément, elles ratent toutes le cache et martèlent votre base de données. La fonction `get_ttl_with_jitter` ajoute une variance aléatoire (10% par défaut) pour étaler les temps d'expiration. Un TTL de 300 secondes devient quelque chose entre 270 et 330 secondes. Combiné avec le pattern cache-aside dans `get_or_set`, cela prévient l'expiration synchronisée à travers vos clés chaudes.\n\n```python\ndef get_ttl_with_jitter(base_ttl: int, jitter_percent: float = 0.1) -> int:\n    jitter = int(base_ttl * jitter_percent)\n    return base_ttl + random.randint(-jitter, jitter)\n```\n\nLa gestion des connexions utilise le pool de connexions async de redis-py avec logique de retry intégrée. La stratégie de retry est `ExponentialWithJitterBackoff` plafonnée à 10 secondes, gérant spécifiquement ConnectionError, TimeoutError et BusyLoadingError. L'invalidation de pattern utilise SCAN au lieu de KEYS parce que KEYS bloque le serveur Redis sur de grands keyspaces. La méthode `invalidate_pattern` itère avec `scan_iter` et supprime les correspondances individuellement. C'est plus lent qu'un seul KEYS + DEL mais ne gelera pas votre Redis de production pendant 30 secondes.",
  "tech_stack": [
    "Python 3.12",
    "FastAPI",
    "Redis",
    "Pydantic v2",
    "hiredis",
    "redis-py async"
  ],
  "github_url": "https://github.com/CarterPerez-dev/fastapi-rc",
  "demo_url": null,
  "website_url": null,
  "docs_url": null,
  "blog_url": null,
  "pypi_url": "https://pypi.org/project/fastapi-rc/",
  "npm_url": null,
  "ios_url": null,
  "android_url": null,
  "code_snippet": "class CacheService(Generic[T]):\n    \"\"\"\n    Generic caching service for Pydantic models\n    Provides cache-aside pattern with automatic serialization\n    \"\"\"\n    def __init__(\n        self,\n        redis: Redis,\n        namespace: str,\n        model: type[T] | None = None,\n        default_ttl: int = 300,\n        use_jitter: bool = True,\n        prefix: str = \"cache\",\n        version: str = \"v1\",\n    ):\n        self.redis = redis\n        self.namespace = namespace\n        self.model = model\n        self.default_ttl = default_ttl\n        self.use_jitter = use_jitter\n        self.prefix = prefix\n        self.version = version\n\n    async def get_or_set(\n        self,\n        identifier: str,\n        factory: Callable[[], Awaitable[T]],\n        ttl: int | None = None,\n        params: dict[str, Any] | None = None,\n    ) -> T:\n        \"\"\"\n        Cache-aside pattern: get from cache or execute factory and cache result\n        \"\"\"\n        cached = await self.get(identifier, params)\n        if cached is not None:\n            return cached\n\n        value = await factory()\n        await self.set(identifier, value, ttl, params)\n        return value",
  "code_language": "python",
  "code_filename": "https://github.com/CarterPerez-dev/fastapi-rc/blob/main/fastapi_rc/service.py",
  "thumbnail_url": null,
  "banner_url": null,
  "screenshots": null,
  "stars_count": 3,
  "forks_count": null,
  "downloads_count": null,
  "users_count": null,
  "display_order": 0,
  "is_complete": true,
  "is_featured": false,
  "status": "active",
  "start_date": "2025-12-13",
  "end_date": null
}
