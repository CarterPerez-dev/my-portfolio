{
  "slug": "vuemantics",
  "language": "fr",
  "title": "Vuemantics",
  "subtitle": "Recherche d'images sémantique utilisant embeddings vectoriels et IA multimodale",
  "description": "J'ai construit ceci pour vraiment comprendre comment fonctionne la recherche sémantique, pas juste appeler une API et espérer le meilleur. L'idée vient de la frustration avec Apple Photos. On est en 2025 et si je cherche 'ami en veste noire' ça ne trouve rien d'utile parce que ça correspond toujours juste aux métadonnées textuelles. Pendant ce temps je peux construire moi-même une vraie recherche sémantique? Quelque chose ne tournait pas rond avec cet écart.",
  "technical_details": "L'architecture enchaîne deux modèles IA ensemble, chacun faisant ce dans quoi il est bon. Gemini Pro gère la vision, analysant les images et vidéos téléchargées pour générer des descriptions textuelles détaillées (ton, sujets, décor, couleurs, actions). Ces descriptions sont ensuite données à text-embedding-3-small d'OpenAI qui les convertit en vecteurs à 1536 dimensions. Les vecteurs vivent dans PostgreSQL via pgvector, et la recherche de similarité se fait à travers des calculs de distance cosinus.\n\n```python\n# Le pipeline central: le modèle vision décrit, le modèle embedding vectorise\ndescription = await self._analyze_image_with_gemini(file_path)\nembedding = await self._generate_embedding(description)\nawait upload.update_analysis(gemini_summary=description, embedding=embedding)\n```\n\nLes requêtes de recherche passent par le même processus d'embedding. Vous tapez 'coucher de soleil à la plage', ce texte devient un vecteur, et pgvector trouve les images dont les vecteurs de description sont les plus proches dans cet espace à 1536 dimensions. Les maths sont juste de la similarité cosinus mais les résultats semblent magiques quand ça fonctionne.\n\n```python\n# Le texte de requête devient un vecteur, pgvector trouve les voisins les plus proches\nquery_embedding = await self._generate_query_embedding(request.query)\nresults = await Upload.search_by_embedding(\n    query_embedding=query_embedding,\n    user_id=user_id,\n    similarity_threshold=request.similarity_threshold\n)\n```\n\nLe backend utilise FastAPI avec traitement entièrement asynchrone. Les téléchargements retournent immédiatement pendant qu'une tâche en arrière-plan gère le pipeline Gemini → OpenAI → pgvector. J'ai choisi asyncpg brut au lieu d'un ORM parce que j'avais besoin d'un contrôle direct sur l'enregistrement du codec de type vecteur. Redis gère la mise en cache. Le frontend est React avec TypeScript, rien de fantaisiste, juste fonctionnel.\n\n```python\n# Codec vecteur personnalisé pour asyncpg - pgvector n'a pas de support Python natif\nawait conn.set_type_codec(\n    \"vector\",\n    encoder=lambda v: f\"[{','.join(map(str, v))}]\",\n    decoder=lambda v: list(map(float, v[1:-1].split(\",\"))),\n    schema=\"public\",\n)\n```\n\nStatut actuel: ça fonctionne, mais nécessite un calibrage de seuil. Il y a un paramètre similarity_threshold (par défaut 0.25) qui contrôle à quel point la correspondance est stricte. Trop bas et vous obtenez des résultats non pertinents, trop haut et vous ratez des correspondances valides. Je l'ai amené à un point d'arrêt décent et je ne suis jamais revenu pour l'affiner correctement. Les parties difficiles sont faites, il ne reste que l'ajustement des paramètres.\n\nProchaine étape est d'exécuter ceci sur mon serveur domestique avec des modèles locaux au lieu d'APIs payantes. Ollama supporte maintenant les modèles vision (llava, llama3.2-vision, qwen2-vl) qui peuvent remplacer Gemini pour générer des descriptions. Pour les embeddings, nomic-embed-text ou mxbai-embed-large fonctionnent localement. La qualité baisse un peu mais pour la recherche média personnelle c'est correct, et puis c'est vraiment gratuit à exécuter. Éventuellement je veux construire une app iOS dessus. Basiquement mes propres iCloud Photos privés qui peuvent vraiment trouver des choses.",
  "tech_stack": [
    "Python",
    "FastAPI",
    "PostgreSQL",
    "pgvector",
    "Redis",
    "React",
    "TypeScript",
    "Vite",
    "Tailwind CSS",
    "OpenAI API",
    "Google Gemini API",
    "Docker",
    "Nginx"
  ],
  "github_url": "https://github.com/CarterPerez-dev/vuemantics",
  "website_url": "https://vuemantics.com",
  "demo_url": null,
  "docs_url": null,
  "blog_url": null,
  "pypi_url": null,
  "npm_url": null,
  "ios_url": null,
  "android_url": null,
  "code_snippet": "async def analyze_media(self, upload_id: UUID) -> None:\n    \"\"\"\n    Analyze media file and generate embeddings.\n    Updates upload record with gemini_summary, embedding, and processing_status.\n    \"\"\"\n    upload = await Upload.find_by_id(upload_id)\n    if not upload:\n        return\n\n    await upload.update_status(ProcessingStatus.ANALYZING)\n\n    # Get description from Gemini (vision model)\n    if upload.file_type == \"image\":\n        description = await self._analyze_image_with_gemini(file_path)\n    else:\n        description = await self._analyze_video_with_gemini(\n            upload.user_id, upload_id, file_path\n        )\n\n    await upload.update_status(ProcessingStatus.EMBEDDING)\n\n    # Convert description to vector embedding (OpenAI)\n    embedding = await self._generate_embedding(description)\n\n    # Store both for search\n    await upload.update_analysis(\n        gemini_summary=description, \n        embedding=embedding\n    )",
  "code_language": "python",
  "code_filename": "https://github.com/CarterPerez-dev/vuemantics/tree/main/backend/services/ai_service.py",
  "thumbnail_url": null,
  "banner_url": null,
  "screenshots": null,
  "stars_count": null,
  "forks_count": null,
  "downloads_count": null,
  "users_count": null,
  "display_order": 0,
  "is_complete": false,
  "is_featured": false,
  "status": "active",
  "start_date": "2025-06-10",
  "end_date": null
}
