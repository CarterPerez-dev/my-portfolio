{
  "slug": "vuemantics",
  "language": "hi",
  "title": "Vuemantics",
  "subtitle": "Vector embeddings और multimodal AI का उपयोग करके semantic image search",
  "description": "मैंने इसे यह समझने के लिए बनाया कि semantic search वास्तव में कैसे काम करता है, न कि बस एक API call करके उम्मीद करना कि सब ठीक हो जाएगा। यह idea Apple Photos से frustration से आया। 2025 है और अगर मैं 'friend in black jacket' search करूं तो कुछ useful नहीं मिलता क्योंकि यह अभी भी बस text metadata match कर रहा है। इस बीच मैं खुद actual semantic search बना सकता हूं? इस gap में कुछ गलत लगा।",
  "technical_details": "Architecture दो AI models को chain करता है, हर एक वो करता है जिसमें वो अच्छा है। Gemini Pro vision handle करता है, uploaded images और videos analyze करके detailed text descriptions generate करता है (tone, subjects, setting, colors, actions)। वो descriptions फिर OpenAI के text-embedding-3-small को feed होते हैं जो उन्हें 1536-dimensional vectors में convert करता है। Vectors PostgreSQL में pgvector के through रहते हैं, और similarity search cosine distance calculations के through होता है।\n\n```python\n# Core pipeline: vision model describe करता है, embedding model vectorize करता है\ndescription = await self._analyze_image_with_gemini(file_path)\nembedding = await self._generate_embedding(description)\nawait upload.update_analysis(gemini_summary=description, embedding=embedding)\n```\n\nSearch queries same embedding process से गुज़रती हैं। आप 'sunset at the beach' type करते हैं, वो text एक vector बनता है, और pgvector उन images को find करता है जिनके description vectors उस 1536-dimensional space में सबसे close हैं। Math बस cosine similarity है पर जब काम करता है तो results magic जैसे लगते हैं।\n\n```python\n# Query text एक vector बनता है, pgvector nearest neighbors find करता है\nquery_embedding = await self._generate_query_embedding(request.query)\nresults = await Upload.search_by_embedding(\n    query_embedding=query_embedding,\n    user_id=user_id,\n    similarity_threshold=request.similarity_threshold\n)\n```\n\nBackend FastAPI use करता है fully async processing के साथ। Uploads immediately return होते हैं जबकि एक background task Gemini → OpenAI → pgvector pipeline handle करता है। मैंने ORM की जगह raw asyncpg लिया क्योंकि मुझे vector type codec registration पर direct control चाहिए था। Redis caching handle करता है। Frontend React है TypeScript के साथ, कुछ fancy नहीं, बस functional।\n\n```python\n# asyncpg के लिए Custom vector codec - pgvector का native Python support नहीं है\nawait conn.set_type_codec(\n    \"vector\",\n    encoder=lambda v: f\"[{','.join(map(str, v))}]\",\n    decoder=lambda v: list(map(float, v[1:-1].split(\",\"))),\n    schema=\"public\",\n)\n```\n\nCurrent status: काम करता है, पर threshold calibration चाहिए। एक similarity_threshold parameter है (default 0.25) जो control करता है कि matching कितनी strict है। Too low और irrelevant results मिलते हैं, too high और valid matches miss हो जाते हैं। मैंने इसे एक decent stopping point तक पहुंचाया और बस properly fine tune करने के लिए कभी वापस नहीं आया। Hard parts done हैं, बस parameter tweaking बाकी है।\n\nNext step है इसे मेरे home server पर paid APIs की जगह local models के साथ run करना। Ollama अब vision models support करता है (llava, llama3.2-vision, qwen2-vl) जो descriptions generate करने के लिए Gemini को replace कर सकते हैं। Embeddings के लिए, nomic-embed-text या mxbai-embed-large locally काम करते हैं। Quality थोड़ी drop होती है पर personal media search के लिए ठीक है, और फिर यह actually free to run है। Eventually मैं इसके ऊपर एक iOS app बनाना चाहता हूं। basically मेरा अपना private iCloud Photos जो actually चीज़ें find कर सके।",
  "tech_stack": [
    "Python",
    "FastAPI",
    "PostgreSQL",
    "pgvector",
    "Redis",
    "React",
    "TypeScript",
    "Vite",
    "Tailwind CSS",
    "OpenAI API",
    "Google Gemini API",
    "Docker",
    "Nginx"
  ],
  "github_url": "https://github.com/CarterPerez-dev/vuemantics",
  "website_url": "https://vuemantics.com",
  "demo_url": null,
  "docs_url": null,
  "blog_url": null,
  "pypi_url": null,
  "npm_url": null,
  "ios_url": null,
  "android_url": null,
  "code_snippet": "async def analyze_media(self, upload_id: UUID) -> None:\n    \"\"\"\n    Analyze media file and generate embeddings.\n    Updates upload record with gemini_summary, embedding, and processing_status.\n    \"\"\"\n    upload = await Upload.find_by_id(upload_id)\n    if not upload:\n        return\n\n    await upload.update_status(ProcessingStatus.ANALYZING)\n\n    # Get description from Gemini (vision model)\n    if upload.file_type == \"image\":\n        description = await self._analyze_image_with_gemini(file_path)\n    else:\n        description = await self._analyze_video_with_gemini(\n            upload.user_id, upload_id, file_path\n        )\n\n    await upload.update_status(ProcessingStatus.EMBEDDING)\n\n    # Convert description to vector embedding (OpenAI)\n    embedding = await self._generate_embedding(description)\n\n    # Store both for search\n    await upload.update_analysis(\n        gemini_summary=description, \n        embedding=embedding\n    )",
  "code_language": "python",
  "code_filename": "https://github.com/CarterPerez-dev/vuemantics/tree/main/backend/services/ai_service.py",
  "thumbnail_url": null,
  "banner_url": null,
  "screenshots": null,
  "stars_count": null,
  "forks_count": null,
  "downloads_count": null,
  "users_count": null,
  "display_order": 0,
  "is_complete": false,
  "is_featured": false,
  "status": "active",
  "start_date": "2025-06-10",
  "end_date": null
}
