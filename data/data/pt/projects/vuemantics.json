{
  "slug": "vuemantics",
  "language": "pt",
  "title": "Vuemantics",
  "subtitle": "Busca semântica de imagens usando embeddings vetoriais e IA multimodal",
  "description": "Construí isso para realmente entender como busca semântica funciona, não apenas chamar uma API e torcer pelo melhor. A ideia veio da frustração com o Apple Photos. É 2025 e se eu buscar 'amigo de jaqueta preta' não encontra nada útil porque ainda está apenas combinando metadados de texto. Enquanto isso eu posso construir busca semântica real eu mesmo? Algo parecia errado sobre essa lacuna.",
  "technical_details": "A arquitetura encadeia dois modelos de IA juntos, cada um fazendo o que faz de melhor. Gemini Pro lida com visão, analisando imagens e vídeos carregados para gerar descrições de texto detalhadas (tom, sujeitos, cenário, cores, ações). Essas descrições são então alimentadas para o text-embedding-3-small da OpenAI que as converte em vetores de 1536 dimensões. Os vetores vivem no PostgreSQL via pgvector, e busca de similaridade acontece através de cálculos de distância de cosseno.\n\n```python\n# O pipeline central: modelo de visão descreve, modelo de embedding vetoriza\ndescription = await self._analyze_image_with_gemini(file_path)\nembedding = await self._generate_embedding(description)\nawait upload.update_analysis(gemini_summary=description, embedding=embedding)\n```\n\nQueries de busca passam pelo mesmo processo de embedding. Você digita 'pôr do sol na praia', esse texto se torna um vetor, e pgvector encontra as imagens cujos vetores de descrição estão mais próximos naquele espaço de 1536 dimensões. A matemática é apenas similaridade de cosseno mas os resultados parecem mágica quando funciona.\n\n```python\n# Texto da query se torna vetor, pgvector encontra vizinhos mais próximos\nquery_embedding = await self._generate_query_embedding(request.query)\nresults = await Upload.search_by_embedding(\n    query_embedding=query_embedding,\n    user_id=user_id,\n    similarity_threshold=request.similarity_threshold\n)\n```\n\nO backend usa FastAPI com processamento totalmente assíncrono. Uploads retornam imediatamente enquanto uma tarefa em background lida com o pipeline Gemini → OpenAI → pgvector. Fui com asyncpg puro ao invés de um ORM porque precisava de controle direto sobre o registro de codec do tipo vector. Redis lida com caching. O frontend é React com TypeScript, nada sofisticado, apenas funcional.\n\n```python\n# Codec vector customizado para asyncpg - pgvector não tem suporte nativo Python\nawait conn.set_type_codec(\n    \"vector\",\n    encoder=lambda v: f\"[{','.join(map(str, v))}]\",\n    decoder=lambda v: list(map(float, v[1:-1].split(\",\"))),\n    schema=\"public\",\n)\n```\n\nStatus atual: funciona, mas precisa de calibração de threshold. Há um parâmetro similarity_threshold (padrão 0.25) que controla quão estrita é a correspondência. Muito baixo e você obtém resultados irrelevantes, muito alto e você perde correspondências válidas. Cheguei a um ponto de parada decente e simplesmente nunca voltei para ajustar adequadamente. As partes difíceis estão feitas, é apenas ajuste de parâmetros que resta.\n\nPróximo passo é rodar isso no meu servidor doméstico com modelos locais ao invés de APIs pagas. Ollama agora suporta modelos de visão (llava, llama3.2-vision, qwen2-vl) que podem substituir o Gemini para gerar descrições. Para embeddings, nomic-embed-text ou mxbai-embed-large funcionam localmente. A qualidade cai um pouco mas para busca de mídia pessoal está ok, e então é realmente grátis para rodar. Eventualmente quero construir um app iOS em cima disso. Basicamente meu próprio iCloud Photos privado que pode realmente encontrar coisas.",
  "tech_stack": [
    "Python",
    "FastAPI",
    "PostgreSQL",
    "pgvector",
    "Redis",
    "React",
    "TypeScript",
    "Vite",
    "Tailwind CSS",
    "OpenAI API",
    "Google Gemini API",
    "Docker",
    "Nginx"
  ],
  "github_url": "https://github.com/CarterPerez-dev/vuemantics",
  "website_url": "https://vuemantics.com",
  "demo_url": null,
  "docs_url": null,
  "blog_url": null,
  "pypi_url": null,
  "npm_url": null,
  "ios_url": null,
  "android_url": null,
  "code_snippet": "async def analyze_media(self, upload_id: UUID) -> None:\n    \"\"\"\n    Analyze media file and generate embeddings.\n    Updates upload record with gemini_summary, embedding, and processing_status.\n    \"\"\"\n    upload = await Upload.find_by_id(upload_id)\n    if not upload:\n        return\n\n    await upload.update_status(ProcessingStatus.ANALYZING)\n\n    # Get description from Gemini (vision model)\n    if upload.file_type == \"image\":\n        description = await self._analyze_image_with_gemini(file_path)\n    else:\n        description = await self._analyze_video_with_gemini(\n            upload.user_id, upload_id, file_path\n        )\n\n    await upload.update_status(ProcessingStatus.EMBEDDING)\n\n    # Convert description to vector embedding (OpenAI)\n    embedding = await self._generate_embedding(description)\n\n    # Store both for search\n    await upload.update_analysis(\n        gemini_summary=description, \n        embedding=embedding\n    )",
  "code_language": "python",
  "code_filename": "https://github.com/CarterPerez-dev/vuemantics/tree/main/backend/services/ai_service.py",
  "thumbnail_url": null,
  "banner_url": null,
  "screenshots": null,
  "stars_count": null,
  "forks_count": null,
  "downloads_count": null,
  "users_count": null,
  "display_order": 0,
  "is_complete": false,
  "is_featured": false,
  "status": "active",
  "start_date": "2025-06-10",
  "end_date": null
}
